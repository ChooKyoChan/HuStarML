{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example DNN code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import lib.dataProcess as dp\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(predict, y): #get a prediction value and real value\n",
    "    result = {}\n",
    "    result['True-Positive'] = 0\n",
    "    result['False-Negative'] = 0\n",
    "    result['True-Negative'] = 0\n",
    "    result['False-Positive'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if int(y[i])==1 : # when the patient has resistant \n",
    "            if int(predict[i]) == 1 : # when the prediction is right\n",
    "                result['True-Positive'] += 1\n",
    "            else :                    # when the prediction is wrong\n",
    "                result['False-Negative'] += 1\n",
    "        else :           #when the patient has sensitive\n",
    "            if int(predict[i]) == 0 : # when the prediction is right\n",
    "                result['True-Negative'] += 1\n",
    "            else :                    # when the prediction is wrong\n",
    "                result['False-Positive'] += 1\n",
    "\n",
    "    # calculate sensitivity\n",
    "    sensitivity=result['True-Positive']/(result['True-Positive']+result['False-Negative'])\n",
    "    \n",
    "    # calculate specificity\n",
    "    specificity=result['True-Negative']/(result['True-Negative']+result['False-Positive'])\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred): #customized loss function \n",
    "\n",
    "    # calculate true positive\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # calculate true negative\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    # calculate false positive\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    # calculate false negative\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "    sen=tp/(tp+fn+K.epsilon())\n",
    "    spe=tn/(tn+fp+K.epsilon())\n",
    "                                #K.epsilon()(very small positive number) is prevent from dividing by 0\n",
    "    # calculate balanced accuracy\n",
    "    bal=(sen+spe)/(2+K.epsilon())\n",
    "    # make nan and zero like number as 0\n",
    "    bal = tf.where(tf.is_nan(bal), tf.zeros_like(bal), bal)\n",
    "\n",
    "    print(tf.size(tp),tf.size(tn))\n",
    "    return 1 - K.mean(bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./TCGA_OV_platinum_status.csv\")\n",
    "\n",
    "# save patient data in 'data_patient'\n",
    "data_patient = data.patient.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>ENSG00000211950</th>\n",
       "      <th>ENSG00000259042</th>\n",
       "      <th>ENSG00000178372</th>\n",
       "      <th>ENSG00000120937</th>\n",
       "      <th>ENSG00000150750</th>\n",
       "      <th>ENSG00000156076</th>\n",
       "      <th>ENSG00000211947</th>\n",
       "      <th>ENSG00000091583</th>\n",
       "      <th>ENSG00000254951</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000185966</th>\n",
       "      <th>ENSG00000211659</th>\n",
       "      <th>ENSG00000187492</th>\n",
       "      <th>ENSG00000166415</th>\n",
       "      <th>ENSG00000244437</th>\n",
       "      <th>ENSG00000064218</th>\n",
       "      <th>ENSG00000243775</th>\n",
       "      <th>ENSG00000188817</th>\n",
       "      <th>ENSG00000211818</th>\n",
       "      <th>Platinum_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-04-1331</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-1.097</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-04-1332</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.959</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-1.959</td>\n",
       "      <td>-1.959</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-04-1347</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-1.124</td>\n",
       "      <td>0.924</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>0.667</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>0.531</td>\n",
       "      <td>1.172</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-04-1348</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.627</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.616</td>\n",
       "      <td>1.533</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-04-1362</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-1.262</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TCGA-04-1364</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>-1.208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TCGA-04-1365</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-1.039</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-1.039</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>1.597</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.783</td>\n",
       "      <td>1.266</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-1.217</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TCGA-04-1514</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-1.341</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TCGA-04-1517</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-1.236</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-1.054</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-1.236</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TCGA-04-1530</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.791</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>1.103</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.854</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TCGA-04-1536</td>\n",
       "      <td>0.733</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TCGA-04-1542</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TCGA-04-1648</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-1.270</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.941</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TCGA-04-1651</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-1.185</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TCGA-04-1655</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TCGA-09-0364</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.893</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TCGA-09-0366</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TCGA-09-1662</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>-1.182</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.527</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TCGA-09-1665</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.659</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TCGA-09-1666</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TCGA-09-1667</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>0.932</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>1.355</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TCGA-09-1670</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TCGA-09-2045</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TCGA-09-2051</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-1.074</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.712</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-1.252</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TCGA-09-2053</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>1.328</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>1.583</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TCGA-09-2056</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TCGA-10-0926</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TCGA-10-0927</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>0.603</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TCGA-10-0931</td>\n",
       "      <td>-1.770</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-1.770</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-1.770</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-1.126</td>\n",
       "      <td>-1.126</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TCGA-10-0933</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>TCGA-31-1953</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>TCGA-36-1568</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>1.041</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-1.204</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>TCGA-36-1570</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-1.047</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-1.225</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.863</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-1.225</td>\n",
       "      <td>1.008</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>TCGA-36-1571</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.486</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>TCGA-36-1574</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.764</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.762</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.761</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>TCGA-36-1576</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>1.135</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.559</td>\n",
       "      <td>1.859</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>TCGA-36-1578</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-1.197</td>\n",
       "      <td>1.967</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>0.983</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>1.533</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>-1.197</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>TCGA-36-1580</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-1.949</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.949</td>\n",
       "      <td>1.162</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.667</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>TCGA-36-1581</td>\n",
       "      <td>1.091</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>1.368</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>-1.111</td>\n",
       "      <td>1.060</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>-1.293</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>1.321</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>0.790</td>\n",
       "      <td>1.681</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>TCGA-57-1586</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.967</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1.258</td>\n",
       "      <td>-1.146</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>TCGA-61-1725</td>\n",
       "      <td>0.846</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-1.783</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>1.221</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-1.783</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.783</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>TCGA-61-1728</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.599</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.292</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>TCGA-61-1733</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-1.019</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>TCGA-61-1736</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.668</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>TCGA-61-1737</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>TCGA-61-1738</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>1.207</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>1.322</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>TCGA-61-1741</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-2.048</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-2.048</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>1.703</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>-1.358</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.048</td>\n",
       "      <td>0.488</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.335</td>\n",
       "      <td>2.048</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>TCGA-61-1743</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>1.559</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>TCGA-61-1907</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>1.119</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>1.232</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>1.068</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>1.577</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>TCGA-61-1910</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>TCGA-61-1911</td>\n",
       "      <td>0.951</td>\n",
       "      <td>-1.748</td>\n",
       "      <td>1.138</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>1.497</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-1.748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>1.419</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>0.849</td>\n",
       "      <td>1.831</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>TCGA-61-1914</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>-0.727</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>1.077</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.534</td>\n",
       "      <td>1.131</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>TCGA-61-1918</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-1.883</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-1.052</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.883</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>TCGA-61-2000</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-1.156</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>TCGA-61-2008</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-1.132</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>TCGA-61-2008</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>1.113</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.992</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>1.527</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>1.378</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>TCGA-61-2009</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>1.078</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>0.839</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.152</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>TCGA-61-2092</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-1.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.055</td>\n",
       "      <td>0.691</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>TCGA-61-2094</td>\n",
       "      <td>2.523</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>2.720</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>2.348</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>1.843</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>TCGA-61-2097</td>\n",
       "      <td>1.399</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-1.187</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          patient  ENSG00000211950  ENSG00000259042  ENSG00000178372  \\\n",
       "0    TCGA-04-1331           -0.492           -1.952            0.438   \n",
       "1    TCGA-04-1332           -0.497           -0.012           -0.327   \n",
       "2    TCGA-04-1347            0.058           -1.768           -0.845   \n",
       "3    TCGA-04-1348            0.194           -1.876            0.015   \n",
       "4    TCGA-04-1362           -1.935           -1.935            0.414   \n",
       "5    TCGA-04-1364           -0.214           -1.861            0.208   \n",
       "6    TCGA-04-1365           -0.515           -0.607            0.395   \n",
       "7    TCGA-04-1514           -2.024           -2.024           -0.373   \n",
       "8    TCGA-04-1517           -0.805           -1.910           -1.910   \n",
       "9    TCGA-04-1530           -0.190           -1.997           -0.265   \n",
       "10   TCGA-04-1536            0.733           -1.681            0.096   \n",
       "11   TCGA-04-1542           -1.662           -1.662           -1.662   \n",
       "12   TCGA-04-1648           -0.252           -0.538           -0.557   \n",
       "13   TCGA-04-1651           -1.843           -0.337            0.109   \n",
       "14   TCGA-04-1655           -1.736           -1.736           -0.823   \n",
       "15   TCGA-09-0364           -0.420           -0.420           -0.625   \n",
       "16   TCGA-09-0366           -1.948           -0.909           -0.293   \n",
       "17   TCGA-09-1662           -0.639           -1.835            0.512   \n",
       "18   TCGA-09-1665            0.084           -0.216           -0.248   \n",
       "19   TCGA-09-1666           -0.550           -0.603           -0.378   \n",
       "20   TCGA-09-1667            0.322           -1.866           -0.796   \n",
       "21   TCGA-09-1670           -0.648           -1.899           -0.193   \n",
       "22   TCGA-09-2045           -1.801           -1.801           -0.011   \n",
       "23   TCGA-09-2051            0.188           -0.393           -0.002   \n",
       "24   TCGA-09-2053            0.212           -0.205           -0.418   \n",
       "25   TCGA-09-2056           -0.459           -0.244            0.311   \n",
       "26   TCGA-10-0926           -0.431           -1.863           -0.231   \n",
       "27   TCGA-10-0927            0.495           -1.695           -1.695   \n",
       "28   TCGA-10-0931           -1.770           -0.665            0.330   \n",
       "29   TCGA-10-0933           -1.805           -1.805            0.100   \n",
       "..            ...              ...              ...              ...   \n",
       "200  TCGA-31-1953           -0.300           -0.893           -0.498   \n",
       "201  TCGA-36-1568            0.217           -0.270            1.041   \n",
       "202  TCGA-36-1570           -0.643           -1.886           -0.342   \n",
       "203  TCGA-36-1571           -1.899           -1.899           -0.341   \n",
       "204  TCGA-36-1574            0.050           -0.558           -0.999   \n",
       "205  TCGA-36-1576            0.705           -0.528            0.398   \n",
       "206  TCGA-36-1578            0.266           -0.398           -1.856   \n",
       "207  TCGA-36-1580           -0.236           -0.466            0.368   \n",
       "208  TCGA-36-1581            1.091           -0.669            1.368   \n",
       "209  TCGA-57-1586           -2.026           -2.026           -0.349   \n",
       "210  TCGA-61-1725            0.846           -0.121            0.690   \n",
       "211  TCGA-61-1728           -0.767           -0.126            0.254   \n",
       "212  TCGA-61-1733           -0.410           -0.731           -1.839   \n",
       "213  TCGA-61-1736           -0.121           -1.908           -1.908   \n",
       "214  TCGA-61-1737            0.025           -0.560            0.367   \n",
       "215  TCGA-61-1738            0.319           -1.798           -1.798   \n",
       "216  TCGA-61-1741           -0.023           -2.048           -0.624   \n",
       "217  TCGA-61-1743           -1.871           -1.871           -0.274   \n",
       "218  TCGA-61-1907            0.401           -1.743            1.119   \n",
       "219  TCGA-61-1910           -1.736           -1.736           -0.194   \n",
       "220  TCGA-61-1911            0.951           -1.748            1.138   \n",
       "221  TCGA-61-1914            0.288           -0.146            0.682   \n",
       "222  TCGA-61-1918           -0.532           -1.883           -0.191   \n",
       "223  TCGA-61-2000           -1.802           -1.802            0.800   \n",
       "224  TCGA-61-2008           -0.278           -1.979            0.217   \n",
       "225  TCGA-61-2008            0.042           -1.864            1.113   \n",
       "226  TCGA-61-2009            0.111           -0.305            1.078   \n",
       "227  TCGA-61-2092           -0.135           -1.688           -1.688   \n",
       "228  TCGA-61-2094            2.523           -0.823           -0.962   \n",
       "229  TCGA-61-2097            1.399           -1.829           -0.083   \n",
       "\n",
       "     ENSG00000120937  ENSG00000150750  ENSG00000156076  ENSG00000211947  \\\n",
       "0             -0.988           -0.909           -0.755            0.296   \n",
       "1             -0.281           -1.083           -0.043            0.048   \n",
       "2             -0.770           -1.768           -1.124            0.924   \n",
       "3             -1.876           -1.037           -0.930            0.627   \n",
       "4             -1.935           -0.668           -0.892           -0.831   \n",
       "5             -1.861           -0.039           -1.861           -1.861   \n",
       "6             -0.855           -1.039           -0.855            0.569   \n",
       "7             -0.774           -0.545            0.049           -0.905   \n",
       "8             -0.754           -1.236           -0.380            0.051   \n",
       "9             -0.589           -1.997           -0.451            0.791   \n",
       "10            -1.681           -1.681           -1.681            0.057   \n",
       "11            -1.662           -1.662           -0.767           -0.412   \n",
       "12            -0.792            0.245           -0.792            0.196   \n",
       "13            -0.415           -0.294           -1.843           -0.114   \n",
       "14            -1.736           -1.736           -0.461           -0.602   \n",
       "15            -0.953           -0.434           -0.765           -0.723   \n",
       "16            -1.096           -1.948           -1.948            0.123   \n",
       "17            -1.835           -0.900           -0.824           -0.225   \n",
       "18            -1.823           -1.823           -0.592            0.659   \n",
       "19            -1.811           -1.811           -0.669           -0.241   \n",
       "20            -0.747           -1.866           -1.866            1.211   \n",
       "21            -1.899           -0.717           -0.390            0.481   \n",
       "22            -1.801           -0.221           -1.801            0.168   \n",
       "23            -1.911           -1.911           -0.571            0.472   \n",
       "24            -1.974           -0.808           -0.499            1.328   \n",
       "25            -1.738           -1.099           -0.566            0.820   \n",
       "26            -0.274           -0.293            0.160           -0.143   \n",
       "27            -1.695           -1.695           -0.579            0.246   \n",
       "28            -0.665            0.070           -1.770            0.105   \n",
       "29            -0.419           -1.805           -0.074           -1.805   \n",
       "..               ...              ...              ...              ...   \n",
       "200           -0.454           -1.920           -1.920            0.024   \n",
       "201           -1.862           -0.920           -0.064            0.260   \n",
       "202           -1.047            0.073           -1.225           -0.939   \n",
       "203           -1.899           -0.453           -0.866           -0.866   \n",
       "204           -1.805           -1.805           -1.805            0.955   \n",
       "205           -0.528           -0.700           -1.911            1.135   \n",
       "206           -1.856           -1.856           -1.197            1.967   \n",
       "207           -1.949           -1.272           -0.901            0.442   \n",
       "208           -0.619           -1.002           -1.111            1.060   \n",
       "209           -2.026           -0.349           -0.396            0.967   \n",
       "210           -0.787           -1.783           -0.347            1.221   \n",
       "211           -0.809           -0.351           -1.967            0.176   \n",
       "212           -0.690           -1.839           -1.839           -0.062   \n",
       "213           -0.766           -0.766           -0.654            0.231   \n",
       "214           -1.865           -1.865           -1.865            0.429   \n",
       "215           -1.798           -0.985           -0.985            1.250   \n",
       "216           -2.048           -0.509           -0.238            1.703   \n",
       "217           -1.037           -0.930           -1.871            0.831   \n",
       "218           -1.743           -0.036           -0.442            1.232   \n",
       "219           -1.736           -1.736           -1.736           -1.736   \n",
       "220           -0.630           -0.490           -0.450            1.497   \n",
       "221           -0.650           -0.727           -0.378            1.077   \n",
       "222           -0.869           -0.869           -0.869           -0.246   \n",
       "223           -1.802           -0.451           -1.802            0.021   \n",
       "224           -1.979           -0.233           -1.132            0.104   \n",
       "225           -0.628           -0.058           -0.340            0.992   \n",
       "226           -1.843           -1.017           -1.843            0.390   \n",
       "227           -1.688           -0.440           -1.688            0.755   \n",
       "228           -0.730           -0.823           -1.920            2.720   \n",
       "229           -0.568           -0.729           -0.396            0.007   \n",
       "\n",
       "     ENSG00000091583  ENSG00000254951  ...  ENSG00000185966  ENSG00000211659  \\\n",
       "0             -1.952           -0.508  ...           -1.952            0.643   \n",
       "1             -0.419           -0.497  ...           -1.959            0.178   \n",
       "2             -0.770           -1.768  ...           -1.768            0.667   \n",
       "3             -1.037           -1.876  ...           -1.876            0.268   \n",
       "4             -0.526           -1.935  ...           -0.613           -0.781   \n",
       "5             -1.031           -1.208  ...           -0.512           -1.861   \n",
       "6             -1.039           -1.877  ...           -1.877            1.597   \n",
       "7             -1.158           -2.024  ...           -2.024           -2.024   \n",
       "8             -1.054           -1.910  ...           -1.910           -0.288   \n",
       "9             -1.997           -1.997  ...           -0.861            1.103   \n",
       "10            -1.681           -0.600  ...           -0.495            0.019   \n",
       "11            -0.099           -0.590  ...           -1.662           -1.662   \n",
       "12            -0.981           -1.270  ...           -1.941            0.076   \n",
       "13            -1.843           -0.823  ...           -1.843            1.012   \n",
       "14            -0.369           -0.748  ...           -1.736           -0.193   \n",
       "15            -0.249           -0.271  ...           -1.908           -1.908   \n",
       "16            -0.987           -1.096  ...           -1.948            0.084   \n",
       "17            -1.006           -1.182  ...           -1.835            0.102   \n",
       "18            -1.823           -1.168  ...           -0.592            0.666   \n",
       "19            -1.811           -0.505  ...           -1.811           -0.419   \n",
       "20            -0.483           -1.214  ...           -1.866            0.932   \n",
       "21            -0.443           -0.648  ...           -1.899           -0.010   \n",
       "22            -1.801           -0.880  ...           -1.801            0.033   \n",
       "23            -0.704           -1.911  ...           -1.911            0.502   \n",
       "24            -1.974           -1.000  ...           -1.974            1.490   \n",
       "25            -0.642           -1.099  ...           -1.738            0.359   \n",
       "26            -1.863           -1.015  ...           -0.829            0.266   \n",
       "27            -0.259           -0.078  ...           -1.695            0.603   \n",
       "28            -1.770           -0.440  ...           -0.952           -0.105   \n",
       "29            -0.869           -0.575  ...           -1.805           -1.805   \n",
       "..               ...              ...  ...              ...              ...   \n",
       "200           -1.920           -0.705  ...           -1.920            0.478   \n",
       "201           -1.862           -1.862  ...           -1.862            0.364   \n",
       "202           -1.886           -0.643  ...           -0.802           -0.503   \n",
       "203           -0.756           -1.899  ...           -1.899           -0.589   \n",
       "204           -0.764           -1.805  ...           -1.805            0.762   \n",
       "205           -1.911           -1.250  ...           -1.911            0.902   \n",
       "206           -0.306           -0.835  ...           -1.856            0.983   \n",
       "207           -0.619           -0.980  ...           -1.949            1.162   \n",
       "208           -1.967           -1.293  ...           -1.967            1.321   \n",
       "209           -2.026           -0.640  ...           -0.212           -0.406   \n",
       "210           -0.007           -1.783  ...           -1.783            0.757   \n",
       "211           -1.110           -0.697  ...           -1.967            0.298   \n",
       "212           -1.839           -0.690  ...           -1.839            0.450   \n",
       "213           -1.908           -0.723  ...           -1.908            0.128   \n",
       "214           -1.865           -1.865  ...           -1.865            0.660   \n",
       "215           -0.075           -1.798  ...           -1.798            1.207   \n",
       "216           -1.060           -1.358  ...           -2.048            0.488   \n",
       "217           -1.214           -1.214  ...           -1.871            1.559   \n",
       "218           -1.743           -0.349  ...           -1.743            1.068   \n",
       "219           -0.272           -0.832  ...           -1.736           -1.736   \n",
       "220           -0.630           -1.748  ...           -0.399            1.419   \n",
       "221           -0.541           -1.191  ...           -1.840            1.400   \n",
       "222           -0.435           -1.052  ...           -1.883            0.039   \n",
       "223           -1.802           -1.156  ...           -1.802           -0.304   \n",
       "224           -1.979           -1.979  ...           -0.298           -0.080   \n",
       "225           -1.864           -1.864  ...           -1.864            1.527   \n",
       "226           -1.843           -0.912  ...           -1.843            0.839   \n",
       "227           -1.688           -1.064  ...           -1.688            0.688   \n",
       "228           -1.920           -0.884  ...           -1.920            2.348   \n",
       "229           -0.729           -1.187  ...           -1.829            0.257   \n",
       "\n",
       "     ENSG00000187492  ENSG00000166415  ENSG00000244437  ENSG00000064218  \\\n",
       "0              0.206           -1.097            0.413           -0.170   \n",
       "1             -1.269           -0.118            0.359           -1.959   \n",
       "2             -0.622            0.531            1.172           -1.768   \n",
       "3             -0.744            0.616            1.533           -0.853   \n",
       "4             -0.168            0.718           -0.362           -0.306   \n",
       "5              0.474            0.516           -0.288           -1.861   \n",
       "6             -0.418            0.783            1.266           -0.326   \n",
       "7             -0.213            0.438           -0.711            0.282   \n",
       "8             -0.205            0.118            0.271           -0.481   \n",
       "9             -0.379            0.233            0.854           -0.096   \n",
       "10            -1.681           -0.420            0.209           -1.050   \n",
       "11            -1.662           -0.767           -1.662           -0.868   \n",
       "12             0.491            0.267            0.449           -0.680   \n",
       "13            -1.843           -0.068           -0.276            0.234   \n",
       "14            -1.736           -0.156           -0.013           -1.736   \n",
       "15            -0.407            0.893           -1.908           -1.908   \n",
       "16            -0.163            0.959            0.185           -1.948   \n",
       "17            -1.835           -0.824            0.527           -0.716   \n",
       "18            -1.823            0.824            0.844            0.054   \n",
       "19            -0.282            0.414            0.137           -0.816   \n",
       "20            -1.214           -1.866            1.355           -0.437   \n",
       "21            -0.165            0.119            0.176           -0.259   \n",
       "22             0.165           -0.253            0.471           -0.308   \n",
       "23            -1.074            0.681            0.712           -0.831   \n",
       "24            -0.267           -0.443            1.583           -1.110   \n",
       "25            -1.738           -1.738            0.566           -1.738   \n",
       "26            -0.325            0.456            0.520           -0.578   \n",
       "27            -0.440           -0.545            0.781           -1.695   \n",
       "28            -0.099            0.129            0.249           -1.126   \n",
       "29            -0.475            0.584           -1.805            0.345   \n",
       "..               ...              ...              ...              ...   \n",
       "200            0.378            0.684            0.525           -0.428   \n",
       "201           -1.026            0.020            0.584           -1.204   \n",
       "202            0.973            0.863           -0.190           -1.886   \n",
       "203            0.509            0.857           -0.321            0.008   \n",
       "204           -0.277           -0.253            0.761           -0.514   \n",
       "205           -0.423            0.559            1.859           -0.411   \n",
       "206           -1.856           -0.070            1.533           -0.775   \n",
       "207           -0.901            0.073            0.667           -0.646   \n",
       "208           -1.002            0.790            1.681           -0.862   \n",
       "209            0.265            0.661            1.258           -1.146   \n",
       "210           -0.203           -0.389            1.389            0.073   \n",
       "211            0.123            0.545            0.599           -1.110   \n",
       "212           -0.329            0.573            0.346           -0.623   \n",
       "213            0.668           -0.229            0.482           -1.908   \n",
       "214           -0.248           -0.705            0.718           -0.129   \n",
       "215           -0.495           -0.291            1.322           -0.659   \n",
       "216           -0.406            0.335            2.048            0.341   \n",
       "217           -0.930            0.317            0.195           -0.607   \n",
       "218            0.283           -0.234            1.577           -0.312   \n",
       "219           -0.383            0.651           -1.736           -0.935   \n",
       "220           -0.774            0.849            1.831           -0.469   \n",
       "221           -0.112            0.534            1.131           -0.224   \n",
       "222           -0.385           -0.307            0.047           -0.623   \n",
       "223           -1.802            0.779            0.141           -0.314   \n",
       "224           -0.051           -0.517            0.173           -0.532   \n",
       "225            0.062           -0.508            1.378           -1.864   \n",
       "226           -0.356            0.333            1.152           -0.543   \n",
       "227            0.180            0.775            1.055            0.691   \n",
       "228           -0.773           -0.318            1.843           -1.920   \n",
       "229           -1.829            0.453            0.038           -0.594   \n",
       "\n",
       "     ENSG00000243775  ENSG00000188817  ENSG00000211818  Platinum_Status  \n",
       "0             -0.798           -0.061           -0.255                0  \n",
       "1             -1.959           -0.557           -0.018                0  \n",
       "2             -0.950           -1.768           -1.768                0  \n",
       "3             -0.853           -0.744           -0.064                0  \n",
       "4             -1.262           -0.179           -0.701                1  \n",
       "5             -0.741            0.398           -1.861                1  \n",
       "6             -1.217           -0.418           -0.704                0  \n",
       "7             -1.341           -0.204           -2.024                0  \n",
       "8             -1.236           -0.368           -1.910                0  \n",
       "9             -1.997           -0.401           -0.401                0  \n",
       "10            -0.880           -1.681           -1.681                0  \n",
       "11            -0.868           -0.201           -1.662                0  \n",
       "12            -1.090            0.602           -0.311                0  \n",
       "13            -1.185           -1.843           -1.843                0  \n",
       "14            -1.736           -0.509           -1.736                0  \n",
       "15            -0.953           -0.256           -0.625                0  \n",
       "16            -0.909            0.114           -0.630                1  \n",
       "17            -0.675           -1.835           -0.291                0  \n",
       "18            -1.823           -1.823           -1.823                0  \n",
       "19            -1.811           -0.125           -0.467                0  \n",
       "20            -1.038           -0.856           -0.079                0  \n",
       "21            -1.234           -0.217           -0.759                0  \n",
       "22            -0.880            0.184           -1.801                0  \n",
       "23            -1.252           -0.551           -0.643                0  \n",
       "24            -1.000           -0.456           -1.974                0  \n",
       "25            -1.738           -0.535           -0.459                0  \n",
       "26            -1.863           -0.086           -1.863                1  \n",
       "27            -1.695           -0.462           -1.695                0  \n",
       "28            -1.126            0.009           -0.772                1  \n",
       "29            -1.805           -0.521           -1.805                0  \n",
       "..               ...              ...              ...              ...  \n",
       "200           -1.257            0.081           -0.300                0  \n",
       "201           -0.920           -0.843           -0.145                0  \n",
       "202           -1.225            1.008           -1.886                1  \n",
       "203           -0.615            0.486           -0.225                1  \n",
       "204           -0.896            0.150           -0.428                0  \n",
       "205           -1.911           -0.304           -0.086                1  \n",
       "206           -1.197           -0.537           -0.537                0  \n",
       "207           -0.839           -0.533           -0.746                0  \n",
       "208           -0.731           -0.402           -0.203                1  \n",
       "209           -0.617            0.470           -0.756                0  \n",
       "210           -0.967           -0.138           -0.243                0  \n",
       "211           -1.292            0.249           -0.860                1  \n",
       "212           -1.019           -0.238           -0.623                0  \n",
       "213           -0.766            0.539           -1.908                0  \n",
       "214           -0.746           -0.483           -0.466                0  \n",
       "215           -0.699           -0.457           -1.798                0  \n",
       "216           -0.917           -0.222           -0.437                0  \n",
       "217           -0.930           -0.516           -0.516                0  \n",
       "218           -0.588            0.250           -0.103                0  \n",
       "219           -0.434           -0.416           -1.736                1  \n",
       "220           -0.774           -0.022           -0.370                0  \n",
       "221           -1.191           -0.146           -1.840                0  \n",
       "222           -0.719           -0.191           -0.316                0  \n",
       "223           -0.695           -1.802           -0.434                0  \n",
       "224           -0.885            0.021           -0.622                0  \n",
       "225           -1.864            0.003           -0.275                0  \n",
       "226           -1.017           -0.356           -0.345                0  \n",
       "227           -0.665            0.080           -1.688                1  \n",
       "228           -0.962           -0.694           -0.001                0  \n",
       "229           -0.910           -0.729           -0.469                0  \n",
       "\n",
       "[230 rows x 202 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Train Data - for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of Training Data\n",
      "Samples : 230\n",
      "Features : 200\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 과정\n",
    "# \"patient\" 컬럼 (축의 내용을 설명하는 부분) 을 제거하고, Input과 Target Data를 분리함\n",
    "x = data.drop([\"patient\",\"Platinum_Status\"],axis=1).values\n",
    "y = data.Platinum_Status.values\n",
    "\n",
    "# 샘플 수와 피쳐 수 출력해보기\n",
    "print(\"Information of Training Data\")\n",
    "print(\"Samples : {}\".format(x.shape[0]))\n",
    "print(\"Features : {}\".format(x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Dataset 나누어주기\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation Dataset 나누어주기\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loss_1/dense_11_loss/Size:0\", shape=(), dtype=int32) Tensor(\"loss_1/dense_11_loss/Size_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    # Densely Connected Layer (Input: 200D, Output: 100D)\n",
    "    tf.keras.layers.Dense(100, input_shape=(200,),activation='relu'),\n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 100D, Output: 80D)\n",
    "    tf.keras.layers.Dense(80, input_shape=(100,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 80D, Output: 60D)\n",
    "    tf.keras.layers.Dense(60, input_shape=(80,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 60D, Output: 40D)\n",
    "    tf.keras.layers.Dense(40, input_shape=(60,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 40D, Output: 20D)\n",
    "    tf.keras.layers.Dense(20, input_shape=(40,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 20D, Output: 1D)\n",
    "    tf.keras.layers.Dense(1, input_shape=(20,),activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# 모델 학습에 필요한 정보들을 명시 (Optimizer, Loss Function, Metric 등)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss=custom_loss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 1ms/sample - loss: 0.4977 - acc: 0.7065\n",
      "184/184 [==============================] - 0s 141us/sample - loss: 0.4977 - acc: 0.7065\n",
      "184/184 [==============================] - 0s 103us/sample - loss: 0.4977 - acc: 0.7065\n",
      "46/46 [==============================] - 0s 134us/sample - loss: 0.4989 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4989 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 129us/sample - loss: 0.4989 - acc: 0.5870\n",
      "Training Sensitivity :  0.04\n",
      "Training Specificity :  0.9552238805970149\n",
      "Validation Sensitivity :  0.0\n",
      "Validation Specificity :  0.9642857142857143\n",
      "Current training Accuracy : 0.70652175\n",
      "Current validation Accuracy : 0.5869565\n",
      "Current training loss : 0.4977294413939766\n",
      "Current validation loss : 0.4989306771236917\n",
      "184/184 [==============================] - 0s 66us/sample - loss: 0.4826 - acc: 0.7446\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.4826 - acc: 0.7446\n",
      "184/184 [==============================] - 0s 55us/sample - loss: 0.4826 - acc: 0.7446\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.5040 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.5040 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.5040 - acc: 0.6087\n",
      "Training Sensitivity :  0.1\n",
      "Training Specificity :  0.9850746268656716\n",
      "Validation Sensitivity :  0.0\n",
      "Validation Specificity :  1.0\n",
      "Current training Accuracy : 0.7445652\n",
      "Current validation Accuracy : 0.6086956\n",
      "Current training loss : 0.48264768849248474\n",
      "Current validation loss : 0.5040223520735035\n",
      "184/184 [==============================] - 0s 67us/sample - loss: 0.4606 - acc: 0.7446\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.4606 - acc: 0.7446\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.4606 - acc: 0.7446\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.5087 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.5087 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.5087 - acc: 0.6087\n",
      "Training Sensitivity :  0.1\n",
      "Training Specificity :  0.9850746268656716\n",
      "Validation Sensitivity :  0.0\n",
      "Validation Specificity :  1.0\n",
      "Current training Accuracy : 0.7445652\n",
      "Current validation Accuracy : 0.6086956\n",
      "Current training loss : 0.4606341330901436\n",
      "Current validation loss : 0.5087202942889669\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.4349 - acc: 0.7663\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.4349 - acc: 0.7663\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.4349 - acc: 0.7663\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.5036 - acc: 0.5652\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.5036 - acc: 0.5652\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.5036 - acc: 0.5652\n",
      "Training Sensitivity :  0.14\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.0\n",
      "Validation Specificity :  0.9285714285714286\n",
      "Current training Accuracy : 0.7663044\n",
      "Current validation Accuracy : 0.5652174\n",
      "Current training loss : 0.43488841212314105\n",
      "Current validation loss : 0.5036304981812186\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.4304 - acc: 0.718 - 0s 86us/sample - loss: 0.4067 - acc: 0.7935\n",
      "184/184 [==============================] - 0s 114us/sample - loss: 0.4067 - acc: 0.7935\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4067 - acc: 0.7935\n",
      "46/46 [==============================] - 0s 103us/sample - loss: 0.4904 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 109us/sample - loss: 0.4904 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 127us/sample - loss: 0.4904 - acc: 0.5870\n",
      "Training Sensitivity :  0.24\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.05555555555555555\n",
      "Validation Specificity :  0.9285714285714286\n",
      "Current training Accuracy : 0.79347825\n",
      "Current validation Accuracy : 0.5869565\n",
      "Current training loss : 0.40666990694792376\n",
      "Current validation loss : 0.4903807458670243\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3784 - acc: 0.7935\n",
      "184/184 [==============================] - 0s 119us/sample - loss: 0.3784 - acc: 0.7935\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.3784 - acc: 0.7935\n",
      "46/46 [==============================] - 0s 191us/sample - loss: 0.4854 - acc: 0.5652\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4854 - acc: 0.5652\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4854 - acc: 0.5652\n",
      "Training Sensitivity :  0.24\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.05555555555555555\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.79347825\n",
      "Current validation Accuracy : 0.5652174\n",
      "Current training loss : 0.3783901463384214\n",
      "Current validation loss : 0.4854337780371956\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.2969 - acc: 0.8478\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.2969 - acc: 0.8478\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2969 - acc: 0.8478\n",
      "46/46 [==============================] - 0s 216us/sample - loss: 0.4761 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 221us/sample - loss: 0.4761 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.4761 - acc: 0.6304\n",
      "Training Sensitivity :  0.46\n",
      "Training Specificity :  0.9925373134328358\n",
      "Validation Sensitivity :  0.2222222222222222\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.84782606\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.2968520625777867\n",
      "Current validation loss : 0.476082374220309\n",
      "184/184 [==============================] - 0s 56us/sample - loss: 0.2415 - acc: 0.8804\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.2415 - acc: 0.8804\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.2415 - acc: 0.8804\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4484 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4484 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4484 - acc: 0.6522\n",
      "Training Sensitivity :  0.64\n",
      "Training Specificity :  0.9701492537313433\n",
      "Validation Sensitivity :  0.2777777777777778\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.8804348\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.24151196687117868\n",
      "Current validation loss : 0.44836208613022516\n",
      "184/184 [==============================] - 0s 114us/sample - loss: 0.2123 - acc: 0.9022\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.2123 - acc: 0.9022\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.2123 - acc: 0.9022\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4366 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 131us/sample - loss: 0.4366 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4366 - acc: 0.6522\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.9701492537313433\n",
      "Validation Sensitivity :  0.2777777777777778\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.90217394\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.21232976084170135\n",
      "Current validation loss : 0.4365537270255711\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1986 - acc: 0.9022\n",
      "184/184 [==============================] - 0s 93us/sample - loss: 0.1986 - acc: 0.9022\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1986 - acc: 0.9022\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4372 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4372 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4372 - acc: 0.6522\n",
      "Training Sensitivity :  0.7\n",
      "Training Specificity :  0.9776119402985075\n",
      "Validation Sensitivity :  0.2777777777777778\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.90217394\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.19863328726395316\n",
      "Current validation loss : 0.43720053589862323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 54us/sample - loss: 0.1385 - acc: 0.9348\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.1385 - acc: 0.9348\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.1385 - acc: 0.9348\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4344 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 199us/sample - loss: 0.4344 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.4344 - acc: 0.6087\n",
      "Training Sensitivity :  0.84\n",
      "Training Specificity :  0.9701492537313433\n",
      "Validation Sensitivity :  0.2777777777777778\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.9347826\n",
      "Current validation Accuracy : 0.6086956\n",
      "Current training loss : 0.13854279725447946\n",
      "Current validation loss : 0.43442285320033197\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.1256 - acc: 0.9457\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.1256 - acc: 0.9457\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.1256 - acc: 0.9457\n",
      "46/46 [==============================] - 0s 121us/sample - loss: 0.4258 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.4258 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4258 - acc: 0.6304\n",
      "Training Sensitivity :  0.84\n",
      "Training Specificity :  0.9850746268656716\n",
      "Validation Sensitivity :  0.2777777777777778\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.9456522\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.12556676760963772\n",
      "Current validation loss : 0.425836153652357\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.0843 - acc: 0.9674\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0843 - acc: 0.9674\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0843 - acc: 0.9674\n",
      "46/46 [==============================] - 0s 92us/sample - loss: 0.3966 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 77us/sample - loss: 0.3966 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3966 - acc: 0.6739\n",
      "Training Sensitivity :  0.92\n",
      "Training Specificity :  0.9850746268656716\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.9673913\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.08426349059395168\n",
      "Current validation loss : 0.3966347160546676\n",
      "184/184 [==============================] - 0s 58us/sample - loss: 0.0556 - acc: 0.9728\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0556 - acc: 0.9728\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0556 - acc: 0.9728\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3953 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 107us/sample - loss: 0.3953 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 151us/sample - loss: 0.3953 - acc: 0.6522\n",
      "Training Sensitivity :  0.98\n",
      "Training Specificity :  0.9701492537313433\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.97282606\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.05561913614687712\n",
      "Current validation loss : 0.39527767378350964\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0420 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0420 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0420 - acc: 0.9783\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3935 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3935 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.3935 - acc: 0.6522\n",
      "Training Sensitivity :  0.98\n",
      "Training Specificity :  0.9776119402985075\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.9782609\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.04201488909514054\n",
      "Current validation loss : 0.39353108406066895\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0370 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0370 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.0370 - acc: 0.9891\n",
      "46/46 [==============================] - 0s 109us/sample - loss: 0.3919 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 98us/sample - loss: 0.3919 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 129us/sample - loss: 0.3919 - acc: 0.6304\n",
      "Training Sensitivity :  0.98\n",
      "Training Specificity :  0.9925373134328358\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.98913044\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.03702200754829075\n",
      "Current validation loss : 0.3919036803038224\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0272 - acc: 0.9946\n",
      "184/184 [==============================] - 0s 59us/sample - loss: 0.0272 - acc: 0.9946\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0272 - acc: 0.9946\n",
      "46/46 [==============================] - 0s 109us/sample - loss: 0.3767 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 123us/sample - loss: 0.3767 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 282us/sample - loss: 0.3767 - acc: 0.6522\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9925373134328358\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.9945652\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.027247128279312798\n",
      "Current validation loss : 0.37668611174044403\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0246 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0246 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 58us/sample - loss: 0.0246 - acc: 0.9891\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3545 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3545 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 129us/sample - loss: 0.3545 - acc: 0.6957\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9850746268656716\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.98913044\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.024648088475932247\n",
      "Current validation loss : 0.35450188232504803\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.0222 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 71us/sample - loss: 0.0222 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.0222 - acc: 0.9837\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3485 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3485 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3485 - acc: 0.6957\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9776119402985075\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.9836956\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.02221673467884893\n",
      "Current validation loss : 0.3485211600428042\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.0357 - acc: 0.9728\n",
      "184/184 [==============================] - 0s 82us/sample - loss: 0.0357 - acc: 0.9728\n",
      "184/184 [==============================] - 0s 80us/sample - loss: 0.0357 - acc: 0.9728\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3339 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3339 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3339 - acc: 0.7391\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9626865671641791\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.97282606\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.035656918650088104\n",
      "Current validation loss : 0.3339452380719392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 87us/sample - loss: 0.0263 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0263 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 59us/sample - loss: 0.0263 - acc: 0.9783\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3318 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 129us/sample - loss: 0.3318 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3318 - acc: 0.6957\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9701492537313433\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.9782609\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.026281185772107994\n",
      "Current validation loss : 0.331796003424603\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0255 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 50us/sample - loss: 0.0255 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 59us/sample - loss: 0.0255 - acc: 0.9783\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3238 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 101us/sample - loss: 0.3238 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 82us/sample - loss: 0.3238 - acc: 0.7391\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9701492537313433\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.9782609\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.025494889072749927\n",
      "Current validation loss : 0.3237879120785257\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0259 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0259 - acc: 0.9783\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0259 - acc: 0.9783\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3169 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 128us/sample - loss: 0.3169 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 98us/sample - loss: 0.3169 - acc: 0.7174\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9701492537313433\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.9782609\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.025938881480175514\n",
      "Current validation loss : 0.3169299467750218\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0237 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0237 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0237 - acc: 0.9837\n",
      "46/46 [==============================] - 0s 153us/sample - loss: 0.3465 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 132us/sample - loss: 0.3465 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 133us/sample - loss: 0.3465 - acc: 0.6957\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9776119402985075\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.9836956\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.02371420549309772\n",
      "Current validation loss : 0.34647459569184674\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0152 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0152 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.0152 - acc: 0.9891\n",
      "46/46 [==============================] - 0s 167us/sample - loss: 0.3754 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 151us/sample - loss: 0.3754 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 151us/sample - loss: 0.3754 - acc: 0.6739\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9850746268656716\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.98913044\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.015184246975442638\n",
      "Current validation loss : 0.37537878233453503\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0189 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0189 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 64us/sample - loss: 0.0189 - acc: 0.9837\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3625 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3625 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3625 - acc: 0.6304\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9776119402985075\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.9836956\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.01894963565080062\n",
      "Current validation loss : 0.3625445158585258\n",
      "184/184 [==============================] - 0s 77us/sample - loss: 0.0121 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 62us/sample - loss: 0.0121 - acc: 0.9891\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.0121 - acc: 0.9891\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3827 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3827 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3827 - acc: 0.6522\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9850746268656716\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.98913044\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.012112677097320557\n",
      "Current validation loss : 0.38265816543413245\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0103 - acc: 0.9946\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0103 - acc: 0.9946\n",
      "184/184 [==============================] - 0s 82us/sample - loss: 0.0103 - acc: 0.9946\n",
      "46/46 [==============================] - 0s 142us/sample - loss: 0.3935 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3935 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3935 - acc: 0.6522\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9925373134328358\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.9945652\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.01027753042138141\n",
      "Current validation loss : 0.3935210963954096\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.0120 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 74us/sample - loss: 0.0120 - acc: 0.9837\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.0120 - acc: 0.9837\n",
      "46/46 [==============================] - 0s 86us/sample - loss: 0.3829 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3829 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3829 - acc: 0.6304\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  0.9776119402985075\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.9836956\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.012021862942239513\n",
      "Current validation loss : 0.38292794123939844\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.0048 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0048 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.0048 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3918 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 120us/sample - loss: 0.3918 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3918 - acc: 0.6739\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.004844997240149457\n",
      "Current validation loss : 0.3918319240860317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0055 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.0055 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.0055 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 157us/sample - loss: 0.3807 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3807 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 105us/sample - loss: 0.3807 - acc: 0.6739\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.0054714757463206415\n",
      "Current validation loss : 0.3807234893674436\n",
      "184/184 [==============================] - 0s 50us/sample - loss: 0.0056 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0056 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.0056 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3594 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 149us/sample - loss: 0.3594 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 147us/sample - loss: 0.3594 - acc: 0.6522\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.0056162346964297085\n",
      "Current validation loss : 0.35937617136084515\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0050 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0050 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 56us/sample - loss: 0.0050 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3619 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3619 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 129us/sample - loss: 0.3619 - acc: 0.6957\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.005009404990984046\n",
      "Current validation loss : 0.3618682596994483\n",
      "184/184 [==============================] - 0s 49us/sample - loss: 0.0040 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0040 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.0040 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3657 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3657 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3657 - acc: 0.6957\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.004022504972374957\n",
      "Current validation loss : 0.3656895678976308\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0026 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.0026 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.0026 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 86us/sample - loss: 0.3649 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3649 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 98us/sample - loss: 0.3649 - acc: 0.6739\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.002619644869928775\n",
      "Current validation loss : 0.3648747724035512\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0039 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0039 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 60us/sample - loss: 0.0039 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 86us/sample - loss: 0.3629 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 109us/sample - loss: 0.3629 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 107us/sample - loss: 0.3629 - acc: 0.7174\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.9285714285714286\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.0039172690847645636\n",
      "Current validation loss : 0.36286273728246277\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.0052 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 71us/sample - loss: 0.0052 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.0052 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3644 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3644 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 129us/sample - loss: 0.3644 - acc: 0.6957\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.005244317262069038\n",
      "Current validation loss : 0.3643887950026471\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0059 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 59us/sample - loss: 0.0059 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0059 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3365 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 124us/sample - loss: 0.3365 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 87us/sample - loss: 0.3365 - acc: 0.7174\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.005928982859072478\n",
      "Current validation loss : 0.3364613781804624\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0045 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.0045 - acc: 1.0000\n",
      "184/184 [==============================] - 0s 54us/sample - loss: 0.0045 - acc: 1.0000\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3316 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 109us/sample - loss: 0.3316 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3316 - acc: 0.7174\n",
      "Training Sensitivity :  1.0\n",
      "Training Specificity :  1.0\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 1.0\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.004505447719408118\n",
      "Current validation loss : 0.3315613736277041\n"
     ]
    }
   ],
   "source": [
    "val_loss_best=100\n",
    "count_lim=15\n",
    "epoch=0\n",
    "\n",
    "train_accuracy_list=[]\n",
    "val_accuracy_list=[]\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "\n",
    "while 1:\n",
    "    # 모델 학습 시키기\n",
    "    model.fit(train_x, train_y, batch_size=10, epochs=1, verbose = 0)\n",
    "    \n",
    "    # Epoch 증가 시키기\n",
    "    epoch+=1\n",
    "    \n",
    "    # Train Dataset에 대한 Loss 값 저장하기\n",
    "    train_loss=model.evaluate(train_x, train_y)[0]\n",
    "    \n",
    "    # Train Dataset에 대한 Accuracy 값 저장하기\n",
    "    train_acc=model.evaluate(train_x, train_y)[1]\n",
    "    curr_training_accuracy=model.evaluate(train_x, train_y)[1]\n",
    "\n",
    "    # Train Dataset에 대한 모델 예측 값 저장하기\n",
    "    tr_predictions = model.predict(train_x)\n",
    "    \n",
    "    # 모델 예측 값이 0.5보다 크면 1로, 아니면 0으로 레이블하기\n",
    "    labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "    \n",
    "    # Vadlidation Dataset에 대한 Loss 값 저장하기\n",
    "    val_loss=model.evaluate(val_x, val_y)[0]\n",
    "    \n",
    "    # Validation Dataset에 대한 Accuracy 값 저장하기\n",
    "    val_acc=model.evaluate(val_x, val_y)[1]\n",
    "    curr_validation_accuracy=model.evaluate(val_x, val_y)[1]\n",
    "    \n",
    "    # Validation Dataset에 대한 모델 예측 값 저장하기\n",
    "    val_predictions = model.predict(val_x)\n",
    "    \n",
    "    # 모델 예측 값이 0.5보다 크면 1로, 아니면 0으로 레이블하기\n",
    "    labeled_val_predictions = np.where(val_predictions > 0.5, 1, 0).flatten()\n",
    "    \n",
    "    # Train 및 Validation Dataset에 대한 Sensitivity, Specificity 계산하기\n",
    "    tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, train_y.tolist())\n",
    "    val_sensitivity, val_specificity = check_correct(labeled_val_predictions, val_y.tolist())\n",
    "    \n",
    "    # 저장한 값들을 출력하기\n",
    "    print(\"Training Sensitivity : \", tr_sensitivity)\n",
    "    print(\"Training Specificity : \", tr_specificity)\n",
    "    \n",
    "    print(\"Validation Sensitivity : \", val_sensitivity)\n",
    "    print(\"Validation Specificity : \", val_specificity)\n",
    "    \n",
    "    print(\"Current training Accuracy : \"+str(curr_training_accuracy))\n",
    "    print(\"Current validation Accuracy : \"+str(curr_validation_accuracy))\n",
    "    \n",
    "    print(\"Current training loss : \"+str(train_loss))\n",
    "    print(\"Current validation loss : \"+str(val_loss))\n",
    "    \n",
    "    # 현재 Epoch에서의 Train 및 Validation Dataset에 대한 Accuracy와 Loss를 리스트에 추가하기\n",
    "    train_accuracy_list.append(train_acc)\n",
    "    val_accuracy_list.append(val_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # 현재 Validation Loss가 이전의 모든 Validation Loss 보다 낮을 경우,\n",
    "    # val_loss_best 변수를 현재 val_loss로 업데이트 하고,\n",
    "    # count 변수를 초기화\n",
    "    if val_loss < val_loss_best: # new best model. count reset.\n",
    "        val_loss_best = val_loss\n",
    "        count=0\n",
    "        \n",
    "    # Validation Loss가 count_lim 횟수 만큼 개선되지 않았을 경우, 학습 중단하기\n",
    "    if count>count_lim: # no increase, stop.\n",
    "        break\n",
    "    \n",
    "    # count 변수 증가시키기\n",
    "    else: \n",
    "        count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6111111111111112, 0.7857142857142857)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sensitivity, val_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train / val loss, Train / val acc 그래프 그려보기\n",
    "fig, loss_aixs = plt.subplots()\n",
    "\n",
    "acc_aixs = loss_aixs.twinx()\n",
    "\n",
    "loss_aixs.plot(train_loss_list, 'y', label='train loss')\n",
    "loss_aixs.plot(val_loss_list, 'r', label='val loss')\n",
    "\n",
    "acc_aixs.plot(train_accuracy_list, 'b', label='train acc')\n",
    "acc_aixs.plot(val_accuracy_list, 'g', label='val acc')\n",
    "\n",
    "loss_aixs.set_xlabel('epoch')\n",
    "loss_aixs.set_ylabel('loss')\n",
    "acc_aixs.set_ylabel('accuracy')\n",
    "\n",
    "loss_aixs.legend(loc='upper left')\n",
    "acc_aixs.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 92us/sample - loss: 0.0722 - acc: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07223690074423085, 0.95108694]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Dataset에 대한 모델의 Loss와 Accuracy 값 리턴\n",
    "model.evaluate(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 152us/sample - loss: 0.2913 - acc: 0.7609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2913026213645935, 0.76086956]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Dataset에 대한 모델의 Loss와 Accuracy 값 리턴\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset에 대해 모델 예측\n",
    "train_h = model.predict(train_x)\n",
    "train_p = (train_h > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset에 대해 모델 예측\n",
    "test_h = model.predict(test_x)\n",
    "test_p = (test_h > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Dataset에 대해 Sensitivity, Specificity 계산하고 출력해보기\n",
    "tr_sensitivity, tr_specificity = check_correct(train_p, train_y)\n",
    "ts_sensitivity, ts_specificity = check_correct(test_p, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94, 0.9552238805970149)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_sensitivity, tr_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5555555555555556, 0.8928571428571429)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_sensitivity, ts_specificity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
